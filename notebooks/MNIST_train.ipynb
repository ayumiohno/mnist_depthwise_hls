{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"SetUp\" and \"Original Definition and Training Sections\" are inspired form <https://mike-12.medium.com/depthwise-separable-convolutions-simple-image-classification-with-pytorch-7f7d2ba06af7>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm8ics1LWXlm"
      },
      "source": [
        "# SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PW8ZQjUcE2YG",
        "outputId": "8abdee5d-f493-448f-ec60-38298fb63e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hwBhuqYxWhy0"
      },
      "outputs": [],
      "source": [
        "from torch import optim, nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "hidden_units = [4, 12]\n",
        "output_units = 10\n",
        "model_d = nn.Sequential(OrderedDict([\n",
        "    ('conv1_depthwise', nn.Conv2d(1, 1, 3, stride=3, padding=1, groups=1, bias=False)),\n",
        "    ('conv1_pointwise', nn.Conv2d(1, hidden_units[0], 1, bias=False)),\n",
        "    ('Relu1', nn.ReLU()),\n",
        "    ('conv2_depthwise', nn.Conv2d(hidden_units[0], hidden_units[0], 3, stride=3, padding=1, groups=hidden_units[0], bias=False)),\n",
        "    ('conv2_pointwise', nn.Conv2d(hidden_units[0], hidden_units[1], 1, bias=False)),\n",
        "    ('Relu2', nn.ReLU()),\n",
        "    ('conv3_depthwise', nn.Conv2d(hidden_units[1], hidden_units[1], 4, stride=4, padding=0, groups=hidden_units[1], bias=False)),\n",
        "    ('conv3_pointwise', nn.Conv2d(hidden_units[1], output_units, 1, bias=False)),\n",
        "    ('log_softmax', nn.LogSoftmax(dim = 1))\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKjnEyPNsXD"
      },
      "source": [
        "# Original Definition and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "id": "y-MR-_a_E9br"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "train = Subset(dataset, torch.arange(10000))\n",
        "test = Subset(dataset, torch.arange(10000, 11024))\n",
        "batch_size = 128\n",
        "trainloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(test, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJffnvyBFAR_",
        "outputId": "30550c53-cbc8-4576-b44b-fd102e1528b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_params: 409\n",
            "0 Training loss: 1.418622894377648\n",
            "1 Training loss: 0.7422349347343927\n",
            "2 Training loss: 0.6520623006398165\n",
            "3 Training loss: 0.6151688664774352\n",
            "4 Training loss: 0.6015881809252727\n",
            "5 Training loss: 0.5960249742375144\n",
            "6 Training loss: 0.5809037153479419\n",
            "7 Training loss: 0.5652959682518923\n",
            "8 Training loss: 0.5657608173316038\n",
            "9 Training loss: 0.558564608610129\n",
            "10 Training loss: 0.5448345394828652\n",
            "11 Training loss: 0.535114960957177\n",
            "12 Training loss: 0.5429439325875873\n",
            "13 Training loss: 0.5405502696580524\n",
            "14 Training loss: 0.5373978999596608\n",
            "15 Training loss: 0.5311610185647313\n",
            "16 Training loss: 0.5293893180316007\n",
            "17 Training loss: 0.5276448485217516\n",
            "18 Training loss: 0.5344730536394482\n",
            "19 Training loss: 0.5300060665305657\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from torch import optim, nn\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "  def forward(self, input):\n",
        "    return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "total_params = 0\n",
        "for parameter in model_d.parameters():\n",
        "  if parameter.requires_grad:\n",
        "    total_params += np.prod(parameter.size())\n",
        "\n",
        "print('total_params:', total_params)\n",
        "\n",
        "model_d.to(device)\n",
        "optimizer_d = optim.Adam(model_d.parameters(), lr = 0.02)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "epochs = 20\n",
        "for i in range(epochs):\n",
        "  running_classification_loss = 0\n",
        "  running_cycle_consistent_loss = 0\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    # Run classification model\n",
        "    predicted_labels = model_d(images)\n",
        "    classification_loss = criterion(Flatten()(predicted_labels), labels)\n",
        "\n",
        "    # Optimize classification weights\n",
        "    classification_loss.backward()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    running_classification_loss += classification_loss.item()\n",
        "    running_loss = running_classification_loss\n",
        "  else:\n",
        "    print(f\"{i} Training loss: {running_loss/len(trainloader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IULq32nFnhy",
        "outputId": "43205f8c-7c8a-44b7-9c5b-c35450dff67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.841796875\n"
          ]
        }
      ],
      "source": [
        "total_correct = 0\n",
        "total_num = 0\n",
        "for images, labels in testloader:\n",
        "  images, labels = images.to(device), labels.to(device)\n",
        "  ps = Flatten()(torch.exp(model_d(images)))\n",
        "  predictions = ps.topk(1, 1, True, True)[1].t()\n",
        "  correct = predictions.eq(labels.view(1, -1))\n",
        "\n",
        "  total_correct += correct.sum().cpu().numpy()\n",
        "  total_num += images.shape[0]\n",
        "\n",
        "print('Accuracy:', total_correct / float(total_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snLwigO4G8Fo",
        "outputId": "a69c4fd2-67b3-4a32-c85e-12090b95faa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1_depthwise.weight torch.Size([1, 1, 3, 3])\n",
            "conv1_pointwise.weight torch.Size([4, 1, 1, 1])\n",
            "conv2_depthwise.weight torch.Size([4, 1, 3, 3])\n",
            "conv2_pointwise.weight torch.Size([12, 4, 1, 1])\n",
            "conv3_depthwise.weight torch.Size([12, 1, 4, 4])\n",
            "conv3_pointwise.weight torch.Size([10, 12, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "for k in model_d.state_dict():\n",
        "  print(k, model_d.state_dict()[k].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYLAVepNNF_I"
      },
      "source": [
        "# Save Model Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJwkBZC_HHpl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# import pickle\n",
        "# params = {}\n",
        "# for k in model_d.state_dict():\n",
        "#   params[k] = model_d.state_dict()[k].cpu().numpy()\n",
        "#   pickle.dump(params[k], open(f'{k}.pk', 'wb'))\n",
        "# pickle.dump(params, open('params.pk', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrL8tocCH062"
      },
      "outputs": [],
      "source": [
        "conv1_depthwise = model_d.state_dict()[\"conv1_depthwise.weight\"].view(1, 3, 3)\n",
        "conv1_pointwise = model_d.state_dict()[\"conv1_pointwise.weight\"].view(4, 1)\n",
        "conv2_depthwise = model_d.state_dict()[\"conv2_depthwise.weight\"].view(4, 3, 3)\n",
        "conv2_pointwise = model_d.state_dict()[\"conv2_pointwise.weight\"].view(12, 4)\n",
        "conv3_depthwise = model_d.state_dict()[\"conv3_depthwise.weight\"].view(12, 4, 4)\n",
        "conv3_pointwise = model_d.state_dict()[\"conv3_pointwise.weight\"].view(10, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L9UzgduIQCJ"
      },
      "outputs": [],
      "source": [
        "params = {}\n",
        "params[\"conv1_depthwise\"] = conv1_depthwise.cpu().numpy().tolist()\n",
        "params[\"conv1_pointwise\"] = conv1_pointwise.cpu().numpy().tolist()\n",
        "params[\"conv2_depthwise\"] = conv2_depthwise.cpu().numpy().tolist()\n",
        "params[\"conv2_pointwise\"] = conv2_pointwise.cpu().numpy().tolist()\n",
        "params[\"conv3_depthwise\"] = conv3_depthwise.cpu().numpy().tolist()\n",
        "params[\"conv3_pointwise\"] = conv3_pointwise.cpu().numpy().tolist()\n",
        "json.dump(params, open('params.json', 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF70fQHhUHzz"
      },
      "outputs": [],
      "source": [
        "torch.save(model_d.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tPZzjb5NrDJ"
      },
      "source": [
        "# Read Model Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pIAGMSw1NsiW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "params = json.load(open('params.json'))\n",
        "conv1_depthwise = torch.tensor(params[\"conv1_depthwise\"])\n",
        "conv1_pointwise = torch.tensor(params[\"conv1_pointwise\"])\n",
        "conv2_depthwise = torch.tensor(params[\"conv2_depthwise\"])\n",
        "conv2_pointwise = torch.tensor(params[\"conv2_pointwise\"])\n",
        "conv3_depthwise = torch.tensor(params[\"conv3_depthwise\"])\n",
        "conv3_pointwise = torch.tensor(params[\"conv3_pointwise\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx4429wzVhWn",
        "outputId": "86f9b9d6-8822-43a9-f0fb-0534067f5cad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_d.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dky3Ma_HNKK-"
      },
      "source": [
        "# Vitis HLS Like Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3gel3y1XWBh"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "e7VZSR6pIyg1"
      },
      "outputs": [],
      "source": [
        "def pointwise(x, weight, size, in_ch, out_ch):\n",
        "  # store weight to local buffer\n",
        "  out = torch.zeros(size, size, out_ch)\n",
        "  for py in range(size):\n",
        "    for px in range(size):\n",
        "      for kp in range(in_ch):\n",
        "        read = x[py, px, kp] # stream in\n",
        "        for l in range(out_ch):\n",
        "            out[py, px, l] += read * weight[l, kp]\n",
        "      for l in range(out_ch):\n",
        "        if out[py, px, l] < 0:\n",
        "          out[py, px, l] = 0\n",
        "        # stream out\n",
        "  return out\n",
        "\n",
        "def depthwise(x, weight, size, in_ch):\n",
        "  next_size = (size+2)//3\n",
        "  out = torch.zeros(next_size, next_size, in_ch)\n",
        "  x_pad = torch.zeros(size+2, size+2, in_ch)\n",
        "  x_pad[1:size+1, 1:size+1, :] = x\n",
        "  for py in range(next_size):\n",
        "    for px in range(next_size):\n",
        "      for l in range(in_ch):\n",
        "        val = 0\n",
        "        for ky in range(3):\n",
        "          for kx in range(3):\n",
        "            val += x_pad[py * 3 + ky, px * 3 + kx, l] * weight[l, ky, kx]\n",
        "        out[py, px, l] = val\n",
        "  return out\n",
        "\n",
        "def depthwise_final(x, weight, size=4, in_ch=16):\n",
        "  # store x, weight to local buffer\n",
        "  next_size = size // 4\n",
        "  out = torch.zeros(next_size, next_size, in_ch)\n",
        "  for l in range(in_ch):\n",
        "    val = 0\n",
        "    for ky in range(4):\n",
        "      for kx in range(4):\n",
        "        val += x[ky, kx, l] * weight[l, ky, kx]\n",
        "    out[0, 0, l] = val\n",
        "    # stream out\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KHdB5_LxOsLS"
      },
      "outputs": [],
      "source": [
        "# depthwise implementation like Vitis HLS\n",
        "def depthwise(x, weight, size, in_ch):\n",
        "  next_size = (size+2)//3\n",
        "  out = torch.zeros(next_size, next_size, in_ch)\n",
        "  line_buf = torch.zeros(2, size+2, in_ch)\n",
        "  window = torch.zeros(3, 3, in_ch)\n",
        "  x = x.view(size * size * in_ch)\n",
        "  x_index = 0\n",
        "  for py in range(next_size):\n",
        "    #### init line_buf[0,:,:] ####\n",
        "    if py != 0:\n",
        "      for l in range(in_ch): # x = 0\n",
        "        line_buf[0, 0, l] = 0\n",
        "      for px in range(1, size+1):\n",
        "        for l in range(in_ch):\n",
        "          line_buf[0, px, l] = x[x_index]\n",
        "          x_index += 1\n",
        "      for l in range(in_ch): # x = -1\n",
        "        line_buf[0, size+1, l] = 0\n",
        "    else:\n",
        "      for px in range(size+2):\n",
        "        for l in range(in_ch):\n",
        "          line_buf[0, px, l] = 0\n",
        "    #### init line_buf[1,:,:] ####\n",
        "    for l in range(in_ch): # x = 0\n",
        "      line_buf[1, 0, l] = 0\n",
        "    for px in range(1, size+1):\n",
        "      for l in range(in_ch):\n",
        "        line_buf[1, px, l] = x[x_index]\n",
        "        x_index += 1\n",
        "    for l in range(in_ch): # x = -1\n",
        "      line_buf[1, size+1, l] = 0\n",
        "    #### iterate ####\n",
        "    for px in range(next_size):\n",
        "        #### set window ####\n",
        "        for ky in range(2):\n",
        "          for kx in range(3):\n",
        "            for l in range(in_ch):\n",
        "              window[ky, kx, l] = line_buf[ky, px * 3 + kx, l]\n",
        "        for kx in range(3):\n",
        "          for l in range(in_ch):\n",
        "            if (px == 0 and kx == 0) or (px == next_size - 1 and kx == 2) or (py == next_size - 1):\n",
        "              window[2, kx, l] = 0\n",
        "            else:\n",
        "              window[2, kx, l] = x[x_index]\n",
        "              x_index += 1\n",
        "        #### convolution ####\n",
        "        for l in range(in_ch):\n",
        "          val = 0\n",
        "          for ky in range(3):\n",
        "            for kx in range(3):\n",
        "              val += window[ky, kx, l] * weight[l, ky, kx]\n",
        "          out[py, px, l] = val # stream out\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2cAqiQV9Q3"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Nghby7SgV5hf"
      },
      "outputs": [],
      "source": [
        "def inf(x):\n",
        "  x1 = depthwise(x, conv1_depthwise, 28, 1)\n",
        "  x2 = pointwise(x1, conv1_pointwise, 10, 1, 4)\n",
        "  x3 = depthwise(x2, conv2_depthwise, 10, 4)\n",
        "  x4 = pointwise(x3, conv2_pointwise, 4, 4, 12)\n",
        "  x5 = depthwise_final(x4, conv3_depthwise, 4, 12)\n",
        "  x6 = pointwise(x5, conv3_pointwise, 1, 12, 10)\n",
        "  return x6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aEwlyFMW6J0",
        "outputId": "04ebc50d-91b7-499e-d591-7733bdd2eca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "Accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "total_correct = 0\n",
        "total_num = 0\n",
        "for image, label in testloader:\n",
        "  image = image.view(28, 28, 1)\n",
        "  res = inf(image)\n",
        "  pred = torch.argmax(res)\n",
        "  if pred == label:\n",
        "    correct = 1\n",
        "  else:\n",
        "    correct = 0\n",
        "  total_correct += correct\n",
        "  total_num += 1\n",
        "  if total_num % 10 == 0:\n",
        "    print(total_num)\n",
        "  if total_num == 100:\n",
        "    break\n",
        "print('Accuracy:', total_correct / float(total_num))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5crE57YEjF"
      },
      "source": [
        "# Prepare Data For PYNQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rA-wWqNRYM30"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "def float_to_int(f):\n",
        "    f = torch.tensor(f, dtype=torch.float16).clone().detach()\n",
        "    packed = struct.pack('>f', f)\n",
        "    unpacked = struct.unpack('>I', packed)[0]\n",
        "    return unpacked\n",
        "\n",
        "# intのビット列をfloatのビット列として解釈する関数\n",
        "def int_to_float(i):\n",
        "    packed = struct.pack('>I', i)\n",
        "    unpacked = struct.unpack('>f', packed)[0]\n",
        "    return unpacked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRYH1c0QYarr",
        "outputId": "c0865e91-7463-454e-a4aa-87628e99afdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-42-5a83a1dfcbcb>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  f = torch.tensor(f, dtype=torch.float16).clone().detach()\n"
          ]
        }
      ],
      "source": [
        "# Store Image and Label\n",
        "images = []     # bit\n",
        "images_py = []  # float\n",
        "labels = []\n",
        "for image, label in testloader:\n",
        "  images_py.append(image.view(1, 1, 28, 28).cpu().numpy().tolist())\n",
        "  image = image.view(28 * 28)\n",
        "  image = [float_to_int(i) for i in image]\n",
        "  label = label.cpu().numpy()[0].item()\n",
        "  images.append(image)\n",
        "  labels.append(label)\n",
        "json.dump(images, open('images.json', 'w'))\n",
        "json.dump(images, open('images_py.json', 'w'))\n",
        "json.dump(labels, open('labels.json', 'w'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
